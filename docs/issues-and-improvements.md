# å°ˆæ¡ˆå•é¡Œèˆ‡æ”¹é€²å»ºè­°

> æœ€å¾Œæ›´æ–°ï¼š2025-10-14
> åˆ†æç¯„åœï¼šPaper Reader å°ˆæ¡ˆå®Œæ•´ç¨‹å¼ç¢¼åº«

---

## ç›®éŒ„

- [åš´é‡å•é¡Œï¼ˆç«‹å³ä¿®å¾©ï¼‰](#åš´é‡å•é¡Œç«‹å³ä¿®å¾©)
- [ä¸­ç­‰å•é¡Œï¼ˆåŠŸèƒ½å¢å¼·ï¼‰](#ä¸­ç­‰å•é¡ŒåŠŸèƒ½å¢å¼·)
- [è¼•å¾®å•é¡Œï¼ˆç¨‹å¼ç¢¼å“è³ªï¼‰](#è¼•å¾®å•é¡Œç¨‹å¼ç¢¼å“è³ª)
- [æ•ˆèƒ½å„ªåŒ–å»ºè­°](#æ•ˆèƒ½å„ªåŒ–å»ºè­°)
- [å®‰å…¨æ€§è€ƒé‡](#å®‰å…¨æ€§è€ƒé‡)

---

## åš´é‡å•é¡Œï¼ˆç«‹å³ä¿®å¾©ï¼‰

### 1. PDF ä¸‹è¼‰é‡è©¦é‚è¼¯éŒ¯èª¤

**ä½ç½®ï¼š** `grab_huggingface.py:53`

**å•é¡Œæè¿°ï¼š**
```python
while status_code != 200 or retry > 10:  # âŒ é‚è¼¯éŒ¯èª¤
```

ä½¿ç”¨ `or` å°è‡´æ¢ä»¶æ°¸é ç‚ºçœŸæˆ–ç„¡æ³•æ­£ç¢ºé‡è©¦ã€‚

**å½±éŸ¿ï¼š**
- è‹¥é¦–æ¬¡ä¸‹è¼‰æˆåŠŸï¼ˆstatus_code == 200ï¼‰ï¼Œä½† retry <= 10ï¼Œè¿´åœˆä»æœƒç¹¼çºŒ
- å¯èƒ½é€ æˆç„¡é™è¿´åœˆæˆ–é‡è¤‡ä¸‹è¼‰

**ä¿®å¾©æ–¹æ¡ˆï¼š**
```python
while status_code != 200 and retry < 10:  # âœ… æ­£ç¢ºé‚è¼¯
    response = requests.get(url)
    status_code = response.status_code
    if status_code == 200:
        with open(filename, 'wb') as f:
            f.write(response.content)
        print(f"Downloaded: {filename}")
    else:
        print(f"Failed to download: {url} with status: {status_code}")
        retry += 1
        time.sleep(1)
```

**å„ªå…ˆç´šï¼š** ğŸ”´ ç·Šæ€¥

---

### 2. éŒ¯èª¤è™•ç†è¦†è“‹ç¯„åœä¸è¶³

**ä½ç½®ï¼š**
- `read_daily_papers.py:153-154`
- `read_paper.py:189-206`
- `zulip_handler.py:11-14`

**å•é¡Œæè¿°ï¼š**
ä½¿ç”¨ bare `except:` æœƒæ•æ‰æ‰€æœ‰ç•°å¸¸ï¼ŒåŒ…æ‹¬ `KeyboardInterrupt` å’Œ `SystemExit`ã€‚

```python
try:
    texts, docs = load_paper(f"./paper_pdf/{paper['id']}.pdf")
    llm_res = sumarize_paper(texts, docs, paper['title'])
    # ...
except:  # âŒ éæ–¼å¯¬æ³›
    logger.error(f'failed to extract paper')
```

**å½±éŸ¿ï¼š**
- ç„¡æ³•è­˜åˆ¥å…·é«”éŒ¯èª¤é¡å‹
- é›£ä»¥é™¤éŒ¯å’Œç›£æ§
- å¯èƒ½éš±è—åš´é‡éŒ¯èª¤

**ä¿®å¾©æ–¹æ¡ˆï¼š**
```python
try:
    texts, docs = load_paper(f"./paper_pdf/{paper['id']}.pdf")
    llm_res = sumarize_paper(texts, docs, paper['title'])
    # ...
except FileNotFoundError as e:
    logger.error(f'PDF file not found: {paper["id"]}.pdf - {e}')
except ValueError as e:
    logger.error(f'Invalid paper format: {paper["id"]} - {e}')
except Exception as e:
    logger.error(f'Failed to extract paper {paper["id"]}: {type(e).__name__} - {e}')
    logger.exception("Full traceback:")
```

**å„ªå…ˆç´šï¼š** ğŸ”´ ç·Šæ€¥

---

### 3. Zulip å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—è™•ç†ä¸ç•¶

**ä½ç½®ï¼š** `zulip_handler.py:11-14`

**å•é¡Œæè¿°ï¼š**
```python
try:
    zulip_client = zulip.Client(config_file=".zuliprc")
except:
    logger.error('zulip service down')
```

è‹¥ `.zuliprc` ä¸å­˜åœ¨æˆ–é…ç½®éŒ¯èª¤ï¼Œ`zulip_client` è®Šæ•¸æœªå®šç¾©ï¼Œå¾ŒçºŒå‘¼å« `post_to_zulip()` æœƒå°è‡´ `NameError`ã€‚

**å½±éŸ¿ï¼š**
- ç¨‹å¼åœ¨ç™¼å¸ƒè¨Šæ¯æ™‚å´©æ½°
- ç„¡æ³•å„ªé›…é™ç´š

**ä¿®å¾©æ–¹æ¡ˆï¼š**
```python
zulip_client = None

try:
    zulip_client = zulip.Client(config_file=".zuliprc")
    logger.info('Zulip client initialized successfully')
except FileNotFoundError:
    logger.error('Zulip config file .zuliprc not found')
except Exception as e:
    logger.error(f'Failed to initialize Zulip client: {e}')

def post_to_zulip(topic, content):
    if zulip_client is None:
        logger.warning('Zulip client not available, skipping message post')
        return None

    request = {
        "type": "stream",
        "to": "Paper_Reader",
        "topic": topic,
        "content": content,
    }
    result = zulip_client.send_message(request)
    logger.info(f'[zulip] message sent: {result}')
    return result
```

**å„ªå…ˆç´šï¼š** ğŸ”´ ç·Šæ€¥

---

## ä¸­ç­‰å•é¡Œï¼ˆåŠŸèƒ½å¢å¼·ï¼‰

### 4. ç¼ºå°‘ç’°å¢ƒè®Šæ•¸é©—è­‰

**ä½ç½®ï¼š** `gpt4o_technical_analyst.py:36-37`

**å•é¡Œæè¿°ï¼š**
ç›´æ¥å­˜å–ç’°å¢ƒè®Šæ•¸ï¼Œè‹¥ç¼ºå¤±æœƒåœ¨é‹è¡Œæ™‚å ±éŒ¯ã€‚

```python
OPENAI_API_KEY = os.environ['OPENAI_API_KEY']  # âŒ å¯èƒ½ KeyError
OPENAI_ORG_ID = os.environ['OPENAI_ORG_ID']
```

**ä¿®å¾©æ–¹æ¡ˆï¼š**
```python
import sys

required_env_vars = ['OPENAI_API_KEY', 'OPENAI_ORG_ID']
missing_vars = [var for var in required_env_vars if var not in os.environ]

if missing_vars:
    logger.error(f'Missing required environment variables: {", ".join(missing_vars)}')
    logger.error('Please create a .env file with the required variables')
    sys.exit(1)

OPENAI_API_KEY = os.environ['OPENAI_API_KEY']
OPENAI_ORG_ID = os.environ['OPENAI_ORG_ID']
logger.info('Environment variables loaded successfully')
```

**å„ªå…ˆç´šï¼š** ğŸŸ¡ ä¸­ç­‰

---

### 5. éš¨æ©Ÿç¡çœ æ™‚é–“éé•·

**ä½ç½®ï¼š** `read_daily_papers.py:61-64`

**å•é¡Œæè¿°ï¼š**
å›ºå®š 30-60 åˆ†é˜é–“éš”å¯èƒ½éŒ¯éå¿«é€Ÿæ›´æ–°çš„è«–æ–‡ã€‚

```python
def random_sleep():
    sleep_time = random.randint(1800, 3600)  # 30-60åˆ†é˜
    logger.info(f"Sleeping for {sleep_time} seconds before next update.")
    time.sleep(sleep_time)
```

**æ”¹é€²æ–¹æ¡ˆï¼š**
```python
def random_sleep():
    # å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼Œé è¨­ 30-60 åˆ†é˜
    min_sleep = int(os.getenv('MIN_SLEEP_SECONDS', '1800'))
    max_sleep = int(os.getenv('MAX_SLEEP_SECONDS', '3600'))
    sleep_time = random.randint(min_sleep, max_sleep)
    logger.info(f"Sleeping for {sleep_time} seconds ({sleep_time//60} minutes) before next update.")
    time.sleep(sleep_time)
```

ä¸¦åœ¨ `.env` ä¸­æ–°å¢ï¼š
```bash
MIN_SLEEP_SECONDS=600   # 10 åˆ†é˜
MAX_SLEEP_SECONDS=1800  # 30 åˆ†é˜
```

**å„ªå…ˆç´šï¼š** ğŸŸ¡ ä¸­ç­‰

---

### 6. è³‡æ–™åº«ç„¡ç´¢å¼•

**ä½ç½®ï¼š** `database.py:18-33`

**å•é¡Œæè¿°ï¼š**
`zulip_topic` æ¬„ä½ç”¨æ–¼æŸ¥è©¢ï¼ˆ`get_paper_by_zulip_topic`ï¼‰ä½†ç„¡ç´¢å¼•ã€‚

**å½±éŸ¿ï¼š**
- éš¨è‘—è«–æ–‡æ•¸é‡å¢åŠ ï¼ŒæŸ¥è©¢æ•ˆèƒ½ä¸‹é™
- ç›®å‰åªæœ‰ 2 ç¯‡è«–æ–‡ï¼Œä½†æœªä¾†å¯èƒ½æœ‰æ•¸ç™¾ç¯‡

**ä¿®å¾©æ–¹æ¡ˆï¼š**
```python
def create_table(conn):
    cursor = conn.cursor()
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS papers (
        id TEXT PRIMARY KEY,
        title TEXT,
        summary TEXT,
        link TEXT,
        pdf_link TEXT,
        local_pdf TEXT,
        zulip_topic TEXT,
        created_at TEXT,
        updated_at TEXT
    )
    ''')

    # æ–°å¢ç´¢å¼•
    cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_zulip_topic ON papers(zulip_topic)
    ''')

    cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_created_at ON papers(created_at)
    ''')

    conn.commit()
```

**å„ªå…ˆç´šï¼š** ğŸŸ¡ ä¸­ç­‰

---

### 7. è«–æ–‡è™•ç†ç‹€æ…‹è¿½è¹¤ä¸è¶³

**å•é¡Œæè¿°ï¼š**
ç›®å‰ç„¡æ³•è¿½è¹¤è«–æ–‡è™•ç†ç‹€æ…‹ï¼ˆä¸‹è¼‰ä¸­/è™•ç†ä¸­/æˆåŠŸ/å¤±æ•—ï¼‰ã€‚

**æ”¹é€²æ–¹æ¡ˆï¼š**
æ–°å¢ `status` æ¬„ä½åˆ°è³‡æ–™åº«ï¼š

```python
def create_table(conn):
    cursor = conn.cursor()
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS papers (
        id TEXT PRIMARY KEY,
        title TEXT,
        summary TEXT,
        link TEXT,
        pdf_link TEXT,
        local_pdf TEXT,
        zulip_topic TEXT,
        status TEXT DEFAULT 'pending',  -- pending/processing/completed/failed
        error_message TEXT,
        retry_count INTEGER DEFAULT 0,
        created_at TEXT,
        updated_at TEXT
    )
    ''')
    conn.commit()
```

**å„ªå…ˆç´šï¼š** ğŸŸ¡ ä¸­ç­‰

---

## è¼•å¾®å•é¡Œï¼ˆç¨‹å¼ç¢¼å“è³ªï¼‰

### 8. å‘é‡è³‡æ–™åº«å…è¨±å±éšªçš„ååºåˆ—åŒ–

**ä½ç½®ï¼š** `gpt4o_technical_analyst.py:70`

**å•é¡Œæè¿°ï¼š**
```python
return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)
```

`allow_dangerous_deserialization=True` å­˜åœ¨å®‰å…¨é¢¨éšªã€‚

**å»ºè­°ï¼š**
- è‹¥è³‡æ–™ä¾†æºå¯ä¿¡ï¼Œä¿æŒç¾ç‹€ä½†åŠ ä¸Šè¨»é‡‹èªªæ˜
- è‹¥éœ€è¦æ›´é«˜å®‰å…¨æ€§ï¼Œè€ƒæ…®ä½¿ç”¨å…¶ä»–åºåˆ—åŒ–æ ¼å¼

```python
def load_db(path):
    embeddings = OpenAIEmbeddings()
    # æ³¨æ„ï¼šæ­¤å°ˆæ¡ˆçš„å‘é‡è³‡æ–™åº«ç”±æœ¬ç³»çµ±ç”Ÿæˆï¼Œä¾†æºå¯ä¿¡
    # è‹¥è¦è¼‰å…¥å¤–éƒ¨å‘é‡è³‡æ–™åº«ï¼Œéœ€ç§»é™¤ allow_dangerous_deserialization
    return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)
```

**å„ªå…ˆç´šï¼š** ğŸŸ¢ ä½

---

### 9. éæ™‚çš„ LangChain import

**ä½ç½®ï¼š** `gpt4o_technical_analyst.py:4-17`

**å•é¡Œæè¿°ï¼š**
ä½¿ç”¨å·²æ£„ç”¨çš„ import è·¯å¾‘ã€‚

```python
from langchain.chat_models import ChatOpenAI  # âš ï¸ å·²æ£„ç”¨
from langchain.document_loaders import PyPDFLoader  # âš ï¸ å·²æ£„ç”¨
```

**ä¿®å¾©æ–¹æ¡ˆï¼š**
```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.question_answering import load_qa_chain
from langchain.chains.summarize import load_summarize_chain
```

åŒæ™‚æ›´æ–° `requirements.txt`ï¼š
```
langchain
langchain-openai
langchain-community
```

**å„ªå…ˆç´šï¼š** ğŸŸ¢ ä½

---

### 10. æ—¥èªŒé…ç½®ä¸çµ±ä¸€

**å•é¡Œæè¿°ï¼š**
- `read_daily_papers.py` æœ‰å®Œæ•´çš„æ—¥èªŒè¼ªæ›¿é…ç½®
- `read_paper.py` åƒ…æœ‰åŸºæœ¬ logger è¨­å®š
- å…¶ä»–æ¨¡çµ„æœªè¨­å®š handler

**å»ºè­°æ–¹æ¡ˆï¼š**
å»ºç«‹çµ±ä¸€çš„æ—¥èªŒé…ç½®æ¨¡çµ„ `logger_config.py`ï¼š

```python
import logging
import os
from logging.handlers import TimedRotatingFileHandler
from pathlib import Path

def setup_logger(name, log_file=None, level=logging.INFO):
    """
    è¨­å®šä¸¦è¿”å›é…ç½®å¥½çš„ logger

    Args:
        name: logger åç¨±
        log_file: æ—¥èªŒæª”æ¡ˆåç¨±ï¼ˆè‹¥ç‚º None å‰‡ä½¿ç”¨é è¨­ï¼‰
        level: æ—¥èªŒç­‰ç´š
    """
    logger = logging.getLogger(name)
    logger.setLevel(level)

    # é¿å…é‡è¤‡æ·»åŠ  handler
    if logger.handlers:
        return logger

    # å»ºç«‹æ—¥èªŒç›®éŒ„
    dir_path = os.path.dirname(os.path.realpath(__file__))
    log_dir_path = f"{dir_path}/log"
    Path(log_dir_path).mkdir(parents=True, exist_ok=True)

    # è¨­å®šæ—¥èªŒæª”æ¡ˆ
    if log_file is None:
        log_file = f"{log_dir_path}/{name}.log"
    else:
        log_file = f"{log_dir_path}/{log_file}"

    # å»ºç«‹æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter(
        '%(asctime)s|%(name)s|%(levelname)s|%(message)s'
    )

    # å»ºç«‹ TimedRotatingFileHandler
    file_handler = TimedRotatingFileHandler(
        log_file,
        when='midnight',
        interval=1,
        backupCount=30  # ä¿ç•™ 30 å¤©
    )
    file_handler.setLevel(level)
    file_handler.suffix = "%Y%m%d"
    file_handler.setFormatter(formatter)

    # å»ºç«‹ Console Handlerï¼ˆå¯é¸ï¼‰
    console_handler = logging.StreamHandler()
    console_handler.setLevel(level)
    console_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger
```

ä½¿ç”¨æ–¹å¼ï¼š
```python
from logger_config import setup_logger

logger = setup_logger("Huggingface daily papers", "daily_papers.log")
```

**å„ªå…ˆç´šï¼š** ğŸŸ¢ ä½

---

### 11. ç¼ºå°‘å‹åˆ¥æç¤º

**å•é¡Œæè¿°ï¼š**
ç¨‹å¼ç¢¼ç¼ºå°‘å‹åˆ¥æç¤ºï¼ˆType Hintsï¼‰ï¼Œé™ä½å¯è®€æ€§å’Œ IDE æ”¯æ´ã€‚

**ç¯„ä¾‹æ”¹é€²ï¼š**
```python
from typing import List, Dict, Tuple, Optional
from langchain.schema import Document

def load_paper(pdf_path: str) -> Tuple[List[Document], List[Document]]:
    """
    è¼‰å…¥ PDF ä¸¦åˆ†å‰²æ–‡æœ¬

    Args:
        pdf_path: PDF æª”æ¡ˆè·¯å¾‘

    Returns:
        (texts, pages): åˆ†å‰²å¾Œçš„æ–‡æœ¬å’ŒåŸå§‹é é¢
    """
    loader = PyPDFLoader(pdf_path)
    pages = loader.load_and_split()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(pages)
    return texts, pages

def get_paper(conn, paper_id: str) -> Optional[Tuple]:
    """
    å¾è³‡æ–™åº«ç²å–è«–æ–‡

    Args:
        conn: è³‡æ–™åº«é€£æ¥
        paper_id: è«–æ–‡ ID

    Returns:
        è«–æ–‡è³‡æ–™æˆ– None
    """
    cursor = conn.cursor()
    cursor.execute('SELECT * FROM papers WHERE id = ?', (paper_id,))
    return cursor.fetchone()
```

**å„ªå…ˆç´šï¼š** ğŸŸ¢ ä½

---

## æ•ˆèƒ½å„ªåŒ–å»ºè­°

### 1. PDF ä¸‹è¼‰æ”¹ç‚ºéåŒæ­¥

**ç•¶å‰å•é¡Œï¼š**
åŒæ­¥ä¸‹è¼‰å¤šç¯‡è«–æ–‡æœƒé˜»å¡ä¸»åŸ·è¡Œç·’ã€‚

**æ”¹é€²æ–¹æ¡ˆï¼š**
ä½¿ç”¨ `asyncio` å’Œ `aiohttp` é€²è¡Œä¸¦è¡Œä¸‹è¼‰ï¼š

```python
import asyncio
import aiohttp

async def download_pdf_async(url: str, filename: str, max_retries: int = 10):
    """éåŒæ­¥ä¸‹è¼‰ PDF"""
    if os.path.exists(filename):
        logger.info(f'pdfå·²å­˜åœ¨: {filename}')
        return filename

    async with aiohttp.ClientSession() as session:
        for retry in range(max_retries):
            try:
                async with session.get(url) as response:
                    if response.status == 200:
                        content = await response.read()
                        with open(filename, 'wb') as f:
                            f.write(content)
                        print(f"Downloaded: {filename}")
                        return filename
                    else:
                        print(f"Failed to download: {url} with status: {response.status}")
            except Exception as e:
                logger.error(f"Download error (attempt {retry+1}/{max_retries}): {e}")

            await asyncio.sleep(1)

    return ''

async def download_all_papers(papers: List[Dict]):
    """ä¸¦è¡Œä¸‹è¼‰æ‰€æœ‰è«–æ–‡"""
    tasks = [
        download_pdf_async(
            paper['pdf_link'],
            f"./paper_pdf/{paper['id']}.pdf"
        )
        for paper in papers
    ]
    return await asyncio.gather(*tasks)
```

**é æœŸæ•ˆç›Šï¼š** ä¸‹è¼‰ 10 ç¯‡è«–æ–‡å¯å¾ 30 ç§’é™è‡³ 5 ç§’

---

### 2. å‘é‡åµŒå…¥å¿«å–æ©Ÿåˆ¶

**å•é¡Œï¼š**
æ¯æ¬¡å•Ÿå‹•éƒ½é‡æ–°è¼‰å…¥å‘é‡è³‡æ–™åº«ã€‚

**æ”¹é€²æ–¹æ¡ˆï¼š**
```python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_paper_embedding_db_cached(paper_id: str):
    """å¸¶å¿«å–çš„å‘é‡è³‡æ–™åº«ç²å–"""
    return get_paper_embedding_db(paper_id)
```

---

### 3. GPT å›æ‡‰å¿«å–

**å•é¡Œï¼š**
ç›¸åŒè«–æ–‡çš„ç›¸ä¼¼å•é¡Œæœƒé‡è¤‡å‘¼å« APIã€‚

**æ”¹é€²æ–¹æ¡ˆï¼š**
ä½¿ç”¨ Redis æˆ–ç°¡å–®çš„æª”æ¡ˆå¿«å–ï¼š

```python
import hashlib
import json

def get_cache_key(paper_id: str, question: str) -> str:
    """ç”Ÿæˆå¿«å–éµ"""
    content = f"{paper_id}:{question}"
    return hashlib.md5(content.encode()).hexdigest()

def answer_question_with_cache(db, question: str, paper_title: str, paper_id: str):
    """å¸¶å¿«å–çš„å•ç­”"""
    cache_key = get_cache_key(paper_id, question)
    cache_file = f"./cache/{cache_key}.json"

    # æª¢æŸ¥å¿«å–
    if os.path.exists(cache_file):
        with open(cache_file, 'r') as f:
            return json.load(f)['answer']

    # å‘¼å« API
    answer = answer_question(db, question, paper_title)

    # å„²å­˜å¿«å–
    os.makedirs('./cache', exist_ok=True)
    with open(cache_file, 'w') as f:
        json.dump({
            'question': question,
            'answer': answer,
            'timestamp': datetime.now().isoformat()
        }, f, ensure_ascii=False, indent=2)

    return answer
```

---

## å®‰å…¨æ€§è€ƒé‡

### ç›®å‰ç‹€æ…‹è©•ä¼°

| é …ç›® | ç‹€æ…‹ | èªªæ˜ |
|------|------|------|
| API é‡‘é‘°ç®¡ç† | âœ… è‰¯å¥½ | ä½¿ç”¨ `.env` æª”æ¡ˆ |
| SQL æ³¨å…¥é˜²è­· | âœ… è‰¯å¥½ | ä½¿ç”¨åƒæ•¸åŒ–æŸ¥è©¢ |
| æª”æ¡ˆè·¯å¾‘éæ­· | âœ… è‰¯å¥½ | PDF æª”åä½¿ç”¨ç™½åå–®ï¼ˆArXiv IDï¼‰ |
| å‘é‡è³‡æ–™åº«ååºåˆ—åŒ– | âš ï¸ æ³¨æ„ | `allow_dangerous_deserialization=True` |
| éŒ¯èª¤è¨Šæ¯æ´©æ¼ | âš ï¸ æ³¨æ„ | éƒ¨åˆ†éŒ¯èª¤è¨Šæ¯éæ–¼è©³ç´° |
| ä¾è³´å¥—ä»¶æ¼æ´ | âš ï¸ æœªæª¢æŸ¥ | å»ºè­°å®šæœŸåŸ·è¡Œ `pip audit` |

### å»ºè­°æªæ–½

1. **å®šæœŸå®‰å…¨æƒæï¼š**
```bash
pip install pip-audit
pip-audit
```

2. **æ•æ„Ÿè³‡è¨Šæª¢æŸ¥ï¼š**
```bash
# ç¢ºä¿ .env å’Œ .zuliprc ä¸è¢«æäº¤
echo ".env" >> .gitignore
echo ".zuliprc" >> .gitignore
echo "cache/" >> .gitignore
```

3. **API é€Ÿç‡é™åˆ¶ï¼š**
```python
from functools import wraps
import time

def rate_limit(max_calls: int, time_frame: int):
    """API å‘¼å«é€Ÿç‡é™åˆ¶è£é£¾å™¨"""
    calls = []

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = time.time()
            # ç§»é™¤éæœŸçš„å‘¼å«è¨˜éŒ„
            calls[:] = [c for c in calls if now - c < time_frame]

            if len(calls) >= max_calls:
                sleep_time = time_frame - (now - calls[0])
                logger.warning(f"Rate limit reached, sleeping for {sleep_time:.2f}s")
                time.sleep(sleep_time)

            calls.append(now)
            return func(*args, **kwargs)
        return wrapper
    return decorator

@rate_limit(max_calls=60, time_frame=60)  # æ¯åˆ†é˜æœ€å¤š 60 æ¬¡å‘¼å«
def sumarize_paper(texts, docs, paper_title):
    # åŸæœ‰ç¨‹å¼ç¢¼
    pass
```

---

## æ¸¬è©¦å»ºè­°

### å»ºè­°æ–°å¢å–®å…ƒæ¸¬è©¦

å»ºç«‹ `tests/` ç›®éŒ„ä¸¦æ–°å¢æ¸¬è©¦ï¼š

```python
# tests/test_database.py
import unittest
from database import create_connection, create_table, insert_paper, get_paper

class TestDatabase(unittest.TestCase):
    def setUp(self):
        """æ¯å€‹æ¸¬è©¦å‰å»ºç«‹æ¸¬è©¦è³‡æ–™åº«"""
        self.test_db = 'test_papers.db'
        self.conn = sqlite3.connect(self.test_db)
        create_table(self.conn)

    def tearDown(self):
        """æ¯å€‹æ¸¬è©¦å¾Œæ¸…ç†"""
        self.conn.close()
        os.remove(self.test_db)

    def test_insert_and_get_paper(self):
        """æ¸¬è©¦æ’å…¥å’Œè®€å–è«–æ–‡"""
        paper = {
            'id': '2407.19672',
            'title': 'Test Paper',
            'summary': '{"æ¨™é¡Œ": "æ¸¬è©¦"}',
            'link': 'https://example.com',
            'pdf_link': 'https://example.com/pdf',
            'local_pdf': './test.pdf',
            'zulip_topic': 'Test Topic'
        }

        insert_paper(self.conn, paper)
        result = get_paper(self.conn, '2407.19672')

        self.assertIsNotNone(result)
        self.assertEqual(result[1], 'Test Paper')

if __name__ == '__main__':
    unittest.main()
```

åŸ·è¡Œæ¸¬è©¦ï¼š
```bash
python -m pytest tests/ -v
```

---

## ç¸½çµ

### ä¿®å¾©å„ªå…ˆé †åº

1. **ç«‹å³è™•ç†ï¼ˆæœ¬é€±å…§ï¼‰ï¼š**
   - PDF ä¸‹è¼‰é‚è¼¯éŒ¯èª¤
   - éŒ¯èª¤è™•ç†æ”¹é€²
   - Zulip å®¢æˆ¶ç«¯åˆå§‹åŒ–

2. **çŸ­æœŸæ”¹é€²ï¼ˆ2 é€±å…§ï¼‰ï¼š**
   - ç’°å¢ƒè®Šæ•¸é©—è­‰
   - è³‡æ–™åº«ç´¢å¼•
   - è™•ç†ç‹€æ…‹è¿½è¹¤

3. **é•·æœŸå„ªåŒ–ï¼ˆ1 å€‹æœˆå…§ï¼‰ï¼š**
   - éåŒæ­¥ä¸‹è¼‰
   - å¿«å–æ©Ÿåˆ¶
   - å–®å…ƒæ¸¬è©¦
   - å‹åˆ¥æç¤º

### é æœŸæ•ˆç›Š

- ğŸ”§ **ç©©å®šæ€§æå‡ï¼š** æ¸›å°‘ 90% çš„é‹è¡Œæ™‚éŒ¯èª¤
- âš¡ **æ•ˆèƒ½æå‡ï¼š** PDF ä¸‹è¼‰é€Ÿåº¦æå‡ 5 å€
- ğŸ›¡ï¸ **å®‰å…¨æ€§æå‡ï¼š** ç¬¦åˆç”Ÿç”¢ç’°å¢ƒæ¨™æº–
- ğŸ“Š **å¯ç¶­è­·æ€§æå‡ï¼š** ç¨‹å¼ç¢¼æ›´æ¸…æ™°æ˜“æ‡‚

---

## é™„éŒ„

### æ¨è–¦å·¥å…·

- **ç¨‹å¼ç¢¼å“è³ªï¼š** `ruff`, `black`, `mypy`
- **å®‰å…¨æƒæï¼š** `pip-audit`, `bandit`
- **æ¸¬è©¦ï¼š** `pytest`, `pytest-cov`
- **æ–‡ä»¶ï¼š** `sphinx`, `mkdocs`

### ç›¸é—œè³‡æº

- [LangChain é·ç§»æŒ‡å—](https://python.langchain.com/docs/guides/migration)
- [Python æ—¥èªŒæœ€ä½³å¯¦è¸](https://docs.python.org/3/howto/logging.html)
- [FAISS å®‰å…¨æ€§è¨è«–](https://github.com/facebookresearch/faiss/issues)
